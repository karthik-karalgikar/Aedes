{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiz5gc+wJyyPRF+pIykrJa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthik-karalgikar/Aedes/blob/main/unet%2B%2Bipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfn9o80IFsTf",
        "outputId": "9dd3b47d-dabc-4762-8634-90f6282beac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# The path below should point to the directory containing this notebook and the associated utility files\n",
        "# Change it if necessary\n",
        "os.chdir('/content/gdrive/MyDrive/Dataset_Segmentation')\n",
        "!ls\n",
        "PATH='/content/drive/My Drive/Dataset_Segmentation/Training'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAdEV449GhAg",
        "outputId": "f95503a9-9302-49e3-adb7-310af21282c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing  Training  Untitled1.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras import backend as keras\n",
        "#from tensorflow import keras\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "FhT5m2j7HaBo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model_new import *\n",
        "img_size=(256,256)"
      ],
      "metadata": {
        "id": "HlTBDqj-HcbU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_class=2"
      ],
      "metadata": {
        "id": "qfW0-uFcH2eG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod = Nest_Net(img_size[0],img_size[1],1,1,deep_supervision=True)"
      ],
      "metadata": {
        "id": "8MRQh-siH72h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Function to generate mask images for all images in a folder\n",
        "def generate_masks(input_folder, output_folder):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Iterate through training and testing folders\n",
        "    for split_folder in os.listdir(input_folder):\n",
        "        split_folder_path = os.path.join(input_folder, split_folder)\n",
        "\n",
        "        # Skip over any non-directory files\n",
        "        if not os.path.isdir(split_folder_path):\n",
        "            continue\n",
        "\n",
        "        # Iterate through each class folder (Aedes and non-aedes)\n",
        "        for class_name in os.listdir(split_folder_path):\n",
        "            class_folder = os.path.join(split_folder_path, class_name)\n",
        "\n",
        "            # Skip over any non-directory files\n",
        "            if not os.path.isdir(class_folder):\n",
        "                continue\n",
        "\n",
        "            # Iterate through each image in the class folder\n",
        "            for filename in os.listdir(class_folder):\n",
        "                # Skip over any non-image files\n",
        "                if not filename.endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "                    continue\n",
        "\n",
        "                # Load the input image\n",
        "                image_path = os.path.join(class_folder, filename)\n",
        "                try:\n",
        "                    image = Image.open(image_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e} - Skipping file: {image_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Determine the class label and generate the corresponding mask\n",
        "                if class_name == 'Aedes':\n",
        "                    mask = np.ones_like(image) * 255  # For Aedes, assign all pixels as 255 (white)\n",
        "                elif class_name == 'Non Aedes':\n",
        "                    mask = np.zeros_like(image)  # For Non Aedes, assign all pixels as 0 (black)\n",
        "\n",
        "                # Convert mask to PIL Image\n",
        "                mask_image = Image.fromarray(mask.astype(np.uint8))\n",
        "\n",
        "                # Save the mask image\n",
        "                mask_filename = os.path.splitext(filename)[0] + \"_mask.png\"  # Example: input_image.png -> input_image_mask.png\n",
        "                mask_image_path = os.path.join(output_folder, split_folder, class_name, mask_filename)\n",
        "                os.makedirs(os.path.join(output_folder, split_folder, class_name), exist_ok=True)\n",
        "                mask_image.save(mask_image_path)\n",
        "\n",
        "# Example usage\n",
        "input_folder = \"/content/gdrive/MyDrive/Dataset_Segmentation\"\n",
        "output_folder = \"/content/gdrive/MyDrive/Mask_Segmentation\"\n",
        "generate_masks(input_folder, output_folder)\n"
      ],
      "metadata": {
        "id": "6JlzypW8K3au"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/gdrive/MyDrive\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ6wznfSLNF6",
        "outputId": "adcb4c75-2298-4954-ddca-cec5d1d80398"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Colab Notebooks'   Dataset_Segmentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.misc as sc\n",
        "\n",
        "\n",
        "def adjustData(img,mask,flag_multi_class,n_class):\n",
        "\n",
        "    if(flag_multi_class):\n",
        "        img /= 255\n",
        "        mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n",
        "        new_mask = np.zeros(mask.shape + (n_class,))\n",
        "        for i in range(n_class):\n",
        "\n",
        "            new_mask[mask == i,i] = 1\n",
        "        new_mask = np.reshape(new_mask,(new_mask.shape[0],new_mask.shape[1]*new_mask.shape[2],new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n",
        "        mask = new_mask\n",
        "    elif(np.max(img)>1):\n",
        "        img = img / 255\n",
        "        mask = mask /255\n",
        "    mask[mask > 0.5] = 1\n",
        "    mask[mask <=0.5] = 0\n",
        "        #print(np.shape(mask),np.shape(img))\n",
        "    return (img,mask)\n",
        "\n",
        "def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n",
        "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
        "                    flag_multi_class = False,n_class = n_class,save_to_dir = None,target_size = img_size,seed = 1):\n",
        "    '''\n",
        "    can generate image and mask at the same time\n",
        "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
        "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
        "    '''\n",
        "    image_datagen = ImageDataGenerator(**aug_dict)\n",
        "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
        "    image_generator = image_datagen.flow_from_directory(\n",
        "        #'./',\n",
        "        train_path,\n",
        "        classes = [image_folder],\n",
        "        color_mode = image_color_mode,\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        save_to_dir = save_to_dir,\n",
        "        save_prefix  = image_save_prefix,\n",
        "        class_mode=None,\n",
        "        seed = seed)\n",
        "    mask_generator = mask_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        classes = [mask_folder],\n",
        "        color_mode = mask_color_mode,\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        save_to_dir = save_to_dir,\n",
        "        save_prefix  = mask_save_prefix,\n",
        "        class_mode=None,\n",
        "        seed = seed)\n",
        "    train_generator = zip(image_generator, mask_generator)\n",
        "    for (img,mask) in train_generator:\n",
        "        img,mask = adjustData(img,mask,flag_multi_class,n_class)\n",
        "        yield (img,mask)\n",
        "\n",
        "def testGenerator(test_path,target_size = img_size,flag_multi_class = True,as_gray = True):\n",
        "    files=sorted(os.listdir(test_path))\n",
        "    num_image=len(files)\n",
        "    for i in range(num_image):\n",
        "        img = io.imread(os.path.join(test_path,files[i]),as_gray = as_gray)\n",
        "        print(files[i])\n",
        "        img = trans.resize(img,target_size)\n",
        "        img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
        "        img = np.reshape(img,(1,)+img.shape)\n",
        "        #print(np.max(img))\n",
        "        yield img"
      ],
      "metadata": {
        "id": "QB-FIJYZICEC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def labelVisualize(num_class,color_dict,img):\n",
        "    img = img[:,:,0] if len(img.shape) == 3 else img\n",
        "    img_out = np.zeros(img.shape + (3,))\n",
        "    for i in range(num_class):\n",
        "        img_out[img == i] = color_dict[i]\n",
        "\n",
        "    return img_out\n",
        "\n",
        "\n",
        "def saveResult(img_path,save_path,npyfile,flag_multi_class = False,num_class = 2):\n",
        "    files=os.listdir(img_path)\n",
        "    #print(len(img_path))\n",
        "    #print(len(npyfile))\n",
        "\n",
        "    for i,item in enumerate(npyfile):\n",
        "        img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
        "        #img1=np.array(((img - np.min(img))/np.ptp(img))>0.6).astype(float)\n",
        "        img[img>0.5]=1\n",
        "        img[img<=0.5]=0\n",
        "        io.imsave(os.path.join(save_path, files[i]),img)\n",
        "\n",
        "\n",
        "\n",
        "def SaveResultwImage(img_path,save_path,npyfile,target_size=img_size,flag_multi_class = False,num_class = 2):\n",
        "    files=os.listdir(img_path)\n",
        "    #print(len(img_path))\n",
        "    #print(len(npyfile))\n",
        "\n",
        "    for i,item in enumerate(npyfile):\n",
        "        img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
        "        #img1=np.array(((img - np.min(img))/np.ptp(img))>0.6).astype(float)\n",
        "        img[img>0.2]=1\n",
        "        img[img<=0.2]=0\n",
        "\n",
        "        I = io.imread(os.path.join(img_path,files[i]), as_gray=True)\n",
        "        I = trans.resize(I,target_size)\n",
        "        #dst = cv2.addWeighted(img, 0.5, I, 0.5, 0.0)\n",
        "        img=np.true_divide((I+img),2)\n",
        "        io.imsave(os.path.join(save_path, files[i]),img)"
      ],
      "metadata": {
        "id": "UJaMubzmIW-K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as sm\n",
        "\n",
        "def get_confusion_matrix_elements(groundtruth_list, predicted_list):\n",
        "    \"\"\"returns confusion matrix elements i.e TN, FP, FN, TP as floats\n",
        "\tSee example code for helper function definitions\n",
        "    \"\"\"\n",
        "    tn, fp, fn, tp = sm.confusion_matrix(groundtruth_list, predicted_list,labels=[0,1]).ravel()\n",
        "    tn, fp, fn, tp = np.float64(tn), np.float64(fp), np.float64(fn), np.float64(tp)\n",
        "\n",
        "    return tn, fp, fn, tp\n",
        "\n",
        "def get_prec_rec_IoU_accuracy(groundtruth_list, predicted_list):\n",
        "    \"\"\"returns precision, recall, IoU and accuracy metrics\n",
        "\t\"\"\"\n",
        "    tn, fp, fn, tp = get_confusion_matrix_elements(groundtruth_list, predicted_list)\n",
        "\n",
        "    total = tp + fp + fn + tn\n",
        "    accuracy = (tp + tn) / total\n",
        "    prec=tp/(tp+fp)\n",
        "    rec=tp/(tp+fn)\n",
        "    IoU=tp/(tp+fp+fn)\n",
        "\n",
        "    return prec,rec,IoU,accuracy\n",
        "\n",
        "def get_f1_score(groundtruth_list, predicted_list):\n",
        "    \"\"\"Return f1 score covering edge cases\"\"\"\n",
        "\n",
        "    tn, fp, fn, tp = get_confusion_matrix_elements(groundtruth_list, predicted_list)\n",
        "\n",
        "    f1_score = (2 * tp) / ((2 * tp) + fp + fn)\n",
        "\n",
        "    return f1_score\n",
        "\n",
        "def get_validation_metrics(groundtruth,predicted):\n",
        "    \"\"\"Return all output metrics. Input is binary images\"\"\"\n",
        "\n",
        "    u,v=np.shape(groundtruth)\n",
        "    groundtruth_list=np.reshape(groundtruth,(u*v,))\n",
        "    predicted_list=np.reshape(predicted,(u*v,))\n",
        "    prec,rec,IoU,acc=get_prec_rec_IoU_accuracy(groundtruth_list, predicted_list)\n",
        "    f1_score=get_f1_score(groundtruth_list, predicted_list)\n",
        "   # print(\"Precision=\",prec, \"Recall=\",rec, \"IoU=\",IoU, \"acc=\",acc, \"F1=\",f1_score)\n",
        "    return prec,rec,IoU,acc,f1_score\n",
        "\n",
        "def evalResult(gth_path,npyfile,target_size=img_size,flag_multi_class = False,num_class = 2):\n",
        "    files=sorted(os.listdir(gth_path))\n",
        "    #print(files)\n",
        "    prec=0\n",
        "    rec=0\n",
        "    acc=0\n",
        "    IoU=0\n",
        "    f1_score=0\n",
        "    for i,item in enumerate(npyfile):\n",
        "        img = item[:,:,0]\n",
        "        gth = io.imread(os.path.join(gth_path,files[i]))\n",
        "        gth = trans.resize(gth,target_size)\n",
        "        if(np.sum(img)>0):\n",
        "            img=np.array(((img - np.min(img))/np.ptp(img))>0.1).astype(float)\n",
        "        if(np.sum(gth)>0):\n",
        "            gth=np.array(((gth - np.min(gth))/np.ptp(gth))>0.1).astype(float)\n",
        "            gth=(gth>0.1).astype(int)\n",
        "        #print(np.shape(gth),np.shape(img))\n",
        "        p,r,I,a,f=get_validation_metrics(gth[:,:,1],img)\n",
        "        if (np.isnan(p)):\n",
        "          p=1\n",
        "        if (np.isnan(r)):\n",
        "          r=1\n",
        "        if (np.isnan(f)):\n",
        "          f=1\n",
        "        if (np.isnan(I)):\n",
        "          I=1\n",
        "        prec=prec+p\n",
        "        rec=rec+r\n",
        "        acc=acc+a\n",
        "        IoU=IoU+I\n",
        "        f1_score=f1_score+f\n",
        "    print(\"Precision=\",prec/(i+1), \"Recall=\",rec/(i+1), \"Jac=\",IoU/(i+1), \"acc=\",acc/(i+1), \"Dice=\",f1_score/(i+1))"
      ],
      "metadata": {
        "id": "mMTRcJUeIbRq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/gdrive/MyDrive/Dataset_Segmentation/Training')\n",
        "data_gen_args = dict(rotation_range=0.2,\n",
        "                     #rescale=1./255,\n",
        "                    width_shift_range=0.2,\n",
        "                    height_shift_range=0.2,\n",
        "                    shear_range=0.1,\n",
        "                    zoom_range=[0.8,1],\n",
        "                    horizontal_flip=True,\n",
        "                    vertical_flip=True,\n",
        "                    fill_mode='nearest')\n",
        "PATH='./train/'"
      ],
      "metadata": {
        "id": "l39uSAUlIji4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_gen = trainGenerator(3,PATH,'/content/gdrive/MyDrive/Dataset_Segmentation/Training/Aedes','/content/gdrive/MyDrive/Mask_Segmentation/Training/Aedes',data_gen_args, save_to_dir = None)"
      ],
      "metadata": {
        "id": "m11ahXSHIuxX"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod.compile(optimizer = Adam(learning_rate=0.0003), loss = dice_coef_loss, metrics = dice_coef)"
      ],
      "metadata": {
        "id": "GZy-pvVcIyXe"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = mod.fit(data_gen,steps_per_epoch=10,epochs=80,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "HNPGYyBdIzee",
        "outputId": "675c68d8-5b88-4d4c-d6ac-f05c17401214"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Aedes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-18328e58ec86>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f4c1e53eced0>\u001b[0m in \u001b[0;36mtrainGenerator\u001b[0;34m(batch_size, train_path, image_folder, mask_folder, aug_dict, image_color_mode, mask_color_mode, image_save_prefix, mask_save_prefix, flag_multi_class, n_class, save_to_dir, target_size, seed)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mimage_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0maug_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmask_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0maug_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     image_generator = image_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m#'./',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Aedes'"
          ]
        }
      ]
    }
  ]
}